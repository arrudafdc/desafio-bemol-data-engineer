# Desafio TÃ©cnico Bemol - Data Engineer

## ğŸ“‹ Sobre o Projeto

Pipeline de ingestÃ£o e processamento de dados utilizando arquitetura Lakehouse com camadas Bronze e Silver. O projeto consome dados da [Fake Store API](https://fakestoreapi.com) (users, carts e products) e realiza transformaÃ§Ãµes progressivas para gerar insights sobre vendas por produto.

**Objetivo Principal:** Criar uma tabela de produtos enriquecida com informaÃ§Ãµes de quantidade total vendida e valor total agregado, alimentada pelos dados de carrinho e preÃ§os de produtos.

---

## ğŸ¯ Arquitetura do Projeto

### Camada Bronze

Tratamento leve dos dados brutos com foco em limpeza estrutural:

- **Notebook 1 (Carts & Products):** Desaninhamento de carrinho com uma linha por produto, remoÃ§Ã£o de nulos e tratamentos bÃ¡sicos
- **Notebook 2 (Users):** PadronizaÃ§Ã£o e limpeza dos dados de usuÃ¡rios

### Camada Silver

AgregaÃ§Ãµes e transformaÃ§Ãµes de negÃ³cio:

- **Notebook 1 (Analytics):** AgregaÃ§Ã£o de quantidade vendida por produto, enriquecimento com preÃ§os e cÃ¡lculo de valor total vendido
- **Notebook 2 (Data Quality):** ValidaÃ§Ãµes e monitoramento de qualidade dos dados

---

## ğŸ› ï¸ Stack TecnolÃ³gico

- **Python** 3.12
- **PySpark** 3.4.2
- **Delta Lake** 2.4.0
- **Requests** (para consumo de API)

> **Nota:** Essas versÃµes foram escolhidas por serem as Ãºnicas que permitiram rodar Delta Lake localmente sem conflitos.

---

## ğŸ“š Arquitetura de Classes

O projeto utiliza programaÃ§Ã£o orientada a objetos para criar uma soluÃ§Ã£o robusta e extensÃ­vel:

### **Lakehouse**

ResponsÃ¡vel por leitura e escrita de dados nas camadas Bronze e Silver, simulando um cenÃ¡rio real de Data Lake.

### **LandingReader**

AbstraÃ§Ã£o para leitura de dados de fontes externas (atualmente implementado para APIs, extensÃ­vel para outras fontes).

### **Logging**

Integrada com `Lakehouse` e `LandingReader`, gera arquivo de logs detalhado sobre todas as operaÃ§Ãµes de leitura e escrita.

### **Monitor**

Complementa o logging gerando:

- Mensagens estruturadas no arquivo de logs
- DataFrame com metadados de escrita (count, nÃºmero de colunas, nome da tabela, timestamp)

### **Controller**

Cria e gerencia colunas de controle com timestamp de processamento, marcando a camada de origem dos dados.

### **Validator**

Realiza validaÃ§Ãµes em dados (validaÃ§Ã£o de email com regex), facilmente extensÃ­vel para outras regras de negÃ³cio.

---

## ğŸš€ Como Usar

### PrÃ©-requisitos

- Python 3.12 instalado
- pip para gerenciamento de dependÃªncias

### InstalaÃ§Ã£o

1. Clone o repositÃ³rio:

```bash
git clone <seu-repositorio>
cd desafio-tecnico-bemol-data-engineer
```

2. Crie um ambiente virtual:

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate  # Windows
```

3. Instale as dependÃªncias:

```bash
pip install pyspark==3.4.2 delta-spark==2.4.0 requests
```

### ExecuÃ§Ã£o

Os notebooks devem ser executados na seguinte ordem:

**Bronze:**

1. `notebooks/bronze/01_carts_products.ipynb` - Desaninhamento e limpeza de carrinho e produtos
2. `notebooks/bronze/02_users.ipynb` - Tratamento de dados de usuÃ¡rios

**Silver:** 3. `notebooks/silver/01_products_analytics.ipynb` - AgregaÃ§Ã£o de vendas por produto 4. `notebooks/silver/02_data_quality.ipynb` - Monitoramento de qualidade

---

## ğŸ“Š SaÃ­da Principal

A tabela gerada no Silver contÃ©m:

| Campo               | DescriÃ§Ã£o                          |
| ------------------- | ---------------------------------- |
| product_id          | ID do produto                      |
| product_name        | Nome do produto                    |
| price               | PreÃ§o unitÃ¡rio                     |
| total_quantity_sold | Quantidade total vendida           |
| total_revenue       | Receita total (preÃ§o Ã— quantidade) |

---

## ğŸ“ Estrutura de DiretÃ³rios

```
desafio-tecnico-bemol-data-engineer/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ 01_carts_products.ipynb
â”‚   â”‚   â””â”€â”€ 02_users.ipynb
â”‚   â””â”€â”€ silver/
â”‚       â”œâ”€â”€ 01_products_analytics.ipynb
â”‚       â””â”€â”€ 02_data_quality.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ lakehouse.py
â”‚   â”œâ”€â”€ landing_reader.py
â”‚   â”œâ”€â”€ logging.py
â”‚   â”œâ”€â”€ monitor.py
â”‚   â”œâ”€â”€ controller.py
â”‚   â””â”€â”€ validator.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/
â”‚   â””â”€â”€ silver/
â”œâ”€â”€ logs/
â”œâ”€â”€ .venv/
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ” Destaques TÃ©cnicos

âœ… **Arquitetura Lakehouse:** SimulaÃ§Ã£o realista de um Data Lake com camadas de refino progressivo

âœ… **CÃ³digo Orientado a Objetos:** ImplementaÃ§Ã£o de classes reutilizÃ¡veis e testÃ¡veis

âœ… **Logging e Monitoramento:** Rastreamento completo de operaÃ§Ãµes com DataFrame de auditoria

âœ… **ValidaÃ§Ã£o de Dados:** Classe Validator com suporte a regex para garantir qualidade

âœ… **Desaninhamento Inteligente:** Explode de dados de carrinho mantendo contexto de cada item

âœ… **Delta Lake Local:** DemonstraÃ§Ã£o de uso de formato open source para versionamento de dados

---

## ğŸ”§ PrÃ³ximos Passos (SugestÃµes)

- Automatizar execuÃ§Ã£o dos notebooks com orquestrador (Airflow, Databricks Workflows)
- Adicionar testes unitÃ¡rios para as classes
- Implementar CI/CD para validaÃ§Ã£o automÃ¡tica
- Expandir LandingReader para suportar mÃºltiplas fontes (CSV, Parquet, Banco de Dados)
- Criar pipeline de testes de qualidade de dados mais robustos

---

## ğŸ“ LicenÃ§a

[Especifique a licenÃ§a, ex: MIT, Apache 2.0, etc.]

---

## ğŸ“§ Contato

[Seu email ou LinkedIn]
